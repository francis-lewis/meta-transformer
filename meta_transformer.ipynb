{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07bbfd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from sairg_utils import launch, \\\n",
    "                        get_data_loaders, \\\n",
    "                        train, \\\n",
    "                        test, \\\n",
    "                        set_random_seeds, \\\n",
    "                        define_finetune_model, \\\n",
    "                        train_process, \\\n",
    "                        builtin_model_initializer, \\\n",
    "                        get_model\n",
    "set_random_seeds()\n",
    "from meta_transformer_vanilla import MetaTransformer, \\\n",
    "                                     metatransformer_model_initializer\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51d9d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ResNet base model using SAIRG infrastructure\n",
    "\n",
    "base_model_weights = torchvision.models.ResNet18_Weights\n",
    "dataset = torchvision.datasets.CIFAR10\n",
    "transforms = base_model_weights.DEFAULT.transforms()\n",
    "\n",
    "model_class = torchvision.models.resnet18\n",
    "model_params = {'pos_params': [],\n",
    "                'key_params': {'weights': base_model_weights},}\n",
    "finetune_params = {'num_classes': 10,\n",
    "                   'head_layer_name': 'fc'}\n",
    "model_params['finetune_params'] = finetune_params\n",
    "loss_fn_class = torch.nn.CrossEntropyLoss\n",
    "optimizer_class = torch.optim.Adam\n",
    "optimizer_params = {'lr': 1e-3}\n",
    "checkpoint_dir = os.path.join(os.getcwd(), \"model_ckpts\")\n",
    "checkpoint_prefix = \"resnet18_cifar10\"\n",
    "\n",
    "base_training_args = {'dataset': dataset,\n",
    "                     'transforms': transforms,\n",
    "                     'num_epochs': 10,\n",
    "                     'batch_size': 64,\n",
    "                     'model_class': model_class,\n",
    "                     'model_params': model_params,\n",
    "                     'model_initializer': builtin_model_initializer,\n",
    "                     'loss_fn_class': loss_fn_class,\n",
    "                     'optimizer_class': optimizer_class,\n",
    "                     'optimizer_params': optimizer_params,\n",
    "                     'checkpoint_dir': checkpoint_dir,\n",
    "                     'checkpoint_prefix': checkpoint_prefix\n",
    "                    }\n",
    "\n",
    "# base_model = get_model(base_training_args, train=True)\n",
    "\n",
    "# train_loader, test_loader = get_data_loaders(dataset, transforms)\n",
    "\n",
    "# base_model.to(device)\n",
    "# acc = test(base_model, device, test_loader)\n",
    "# print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f22c8162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MetaNet\n",
    "# 725s, 86%, ZeRO + 2 layers\n",
    "# 191s, 85%, 1 thr + 2 layers\n",
    "base_model_weights = torchvision.models.ResNet18_Weights\n",
    "dataset = torchvision.datasets.CIFAR10\n",
    "transforms = base_model_weights.DEFAULT.transforms()\n",
    "\n",
    "model_class = MetaTransformer\n",
    "base_model = get_model(base_training_args, train=True)\n",
    "train_loader, test_loader = get_data_loaders(dataset, transforms)\n",
    "layer_names = [k for k, v in base_model.named_modules() if 'layer' in k and '.' not in k] + ['fc']\n",
    "del base_model\n",
    "input_batch, _ = iter(train_loader).next()\n",
    "base_model_checkpoint_path = os.path.join(base_training_args['checkpoint_dir'],\n",
    "                                          base_training_args['checkpoint_prefix'])\n",
    "model_params = {'layer_names': layer_names,\n",
    "                'input_batch': input_batch.shape,\n",
    "                'base_training_args': base_training_args,\n",
    "                'kwargs': {'num_transformer_layers': 2}}\n",
    "loss_fn_class = torch.nn.CrossEntropyLoss\n",
    "optimizer_class = torch.optim.Adam\n",
    "optimizer_params = {'lr': 1e-4}\n",
    "checkpoint_dir = os.path.join(os.getcwd(), \"model_ckpts\")\n",
    "checkpoint_prefix = \"meta_transformer_resnet18_cifar10\"\n",
    "\n",
    "meta_training_args = {'dataset': dataset,\n",
    "                     'transforms': transforms,\n",
    "                     'num_epochs': 1,\n",
    "                     'batch_size': 64,\n",
    "                     'model_class': model_class,\n",
    "                     'model_params': model_params,\n",
    "                     'model_initializer': metatransformer_model_initializer,\n",
    "                     'loss_fn_class': loss_fn_class,\n",
    "                     'optimizer_class': optimizer_class,\n",
    "                     'optimizer_params': optimizer_params,\n",
    "                     'checkpoint_dir': checkpoint_dir,\n",
    "                     'checkpoint_prefix': checkpoint_prefix\n",
    "                    }\n",
    "\n",
    "# meta_model = get_model(meta_training_args, train=True)\n",
    "meta_model = get_model(meta_training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cebd3a5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train process rank 0\n",
      "base_training_args: {'dataset': <class 'torchvision.datasets.cifar.CIFAR10'>, 'transforms': ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      "), 'num_epochs': 10, 'batch_size': 64, 'model_class': <function resnet18 at 0x7f8ee5133dd0>, 'model_params': {'pos_params': [], 'key_params': {'weights': <enum 'ResNet18_Weights'>}, 'finetune_params': {'num_classes': 10, 'head_layer_name': 'fc'}}, 'model_initializer': <function builtin_model_initializer at 0x7f8ee4f81950>, 'loss_fn_class': <class 'torch.nn.modules.loss.CrossEntropyLoss'>, 'optimizer_class': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.001}, 'checkpoint_dir': '/home/francis/Developer/experiments/meta-transformer/model_ckpts', 'checkpoint_prefix': 'resnet18_cifar10'}\n",
      "[1,   156] loss: 0.831\n",
      "[1,   312] loss: 0.637\n",
      "[1,   468] loss: 0.581\n",
      "[1,   624] loss: 0.578\n",
      "[1,   780] loss: 0.535\n",
      "Training epoch 1 took 157.3321497440338s\n",
      "Saving meta_transformer_resnet18_cifar10.pth at epoch 1\n",
      "Accuracy after epoch 1: 81.03 %\n",
      "Time to train all epochs was 179.60435938835144s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "184.67360830307007"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = time.time()\n",
    "# launch(meta_training_args, num_proc=2)\n",
    "# train_process(0, meta_training_args)\n",
    "tik = time.time()\n",
    "tik - tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b344bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66d6cddd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# base_net_weights = torchvision.models.ResNet18_Weights.DEFAULT\n",
    "# base_net = torchvision.models.resnet18(weights=base_net_weights)\n",
    "# module_list = [k for k, v in base_net.named_modules() if 'layer' in k and '.' not in k]\n",
    "# # module_list = [k for k, v in base_net.named_modules() if 'bn' in k]\n",
    "# module_list += ['fc']\n",
    "# print(len(module_list))\n",
    "# module_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba4d11d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(base_net_weights)\n",
    "# base_net_weights == torchvision.models.ResNet18_Weights.DEFAULT\n",
    "# type(base_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fe4712e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# base_net_weights = torchvision.models.ViT_B_32_Weights.DEFAULT\n",
    "# base_net = torchvision.models.vit_b_32(weights=base_net_weights)\n",
    "# # [k for k, v in base_net.named_modules() if 'encoder_layer' in k and len(k.split('.')) < 4]\n",
    "# # [k for k, v in base_net.named_modules()]\n",
    "# dict(base_net.named_modules())['heads']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4b1fa67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataset = torchvision.datasets.CIFAR10\n",
    "# transform = base_net_weights.transforms()\n",
    "# train_loader, test_loader = get_data_loaders(dataset, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb806af8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(getattr(base_net, 'fc'))\n",
    "# base_net = define_finetune_model(base_net, 10, 'fc', finetune_base=False)\n",
    "# print(getattr(base_net, 'fc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2d7619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ckpt_dir = os.path.expanduser('~/Developer/experiments/meta-transformer/model_ckpts')\n",
    "# base_model_filename = 'resnet18_cifar10.pth'\n",
    "# base_model_path = os.path.join(model_ckpt_dir, base_model_filename)\n",
    "# if os.path.exists(base_model_path):\n",
    "#     base_net.load_state_dict(torch.load(base_model_path))\n",
    "# else:\n",
    "#     loss_fn = torch.nn.CrossEntropyLoss()\n",
    "#     optimizer = torch.optim.Adam(base_net.parameters(), lr=1e-3)\n",
    "#     base_net.to(device)\n",
    "#     tik = time.time()\n",
    "#     loss = train(base_net, loss_fn, optimizer, device, train_loader)\n",
    "#     tok = time.time()\n",
    "#     print(loss)\n",
    "#     print(tok - tik)\n",
    "#     torch.save(base_net.state_dict(), base_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2861df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_net.to(device)\n",
    "# acc = test(base_net, device, test_loader)\n",
    "# print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3ab8a33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# images, _ = iter(train_loader).next()\n",
    "# images = images.to(device)\n",
    "# num_layers = 2\n",
    "# meta_net = MetaTransformer(base_net, module_list, images, num_transformer_layers=num_layers)\n",
    "# meta_net.to(device)\n",
    "# meta_net(images).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c19dbf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do_reload = True\n",
    "# base_model_filename = 'meta_transformer_resnet18_cifar10.pth'\n",
    "# base_model_path = os.path.join(model_ckpt_dir, base_model_filename)\n",
    "# if do_reload and os.path.exists(base_model_path):\n",
    "#     meta_net.load_state_dict(torch.load(base_model_path))\n",
    "# else:\n",
    "#     loss_fn = torch.nn.CrossEntropyLoss()\n",
    "#     optimizer = torch.optim.Adam(meta_net.parameters(), lr=1e-4)\n",
    "#     meta_net.to(device)\n",
    "#     tik = time.time()\n",
    "#     loss = train(meta_net, loss_fn, optimizer, device, train_loader)\n",
    "#     tok = time.time()\n",
    "#     print(loss)\n",
    "#     print(tok - tik)\n",
    "#     torch.save(meta_net.state_dict(), base_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3af4eaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_net.to(device)\n",
    "# acc = test(meta_net, device, test_loader)\n",
    "# print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3432139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_net.eval()\n",
    "# meta_net(images)\n",
    "\n",
    "# acts = meta_net._activations['fc']\n",
    "# vals, inds = torch.max(acts, 1)\n",
    "# vals = torch.unsqueeze(vals, 1)\n",
    "# inds = torch.unsqueeze(inds, 1).repeat((1, 10))\n",
    "# src = torch.tensor(range(100)).reshape((10, 10))\n",
    "# src = src.unsqueeze(0).repeat((64, 1, 1))\n",
    "# torch.scatter(torch.zeros_like(acts), 1, inds, vals)\n",
    "\n",
    "# print(acts.shape)\n",
    "# res = acts[inds == 6].reshape((-1, 10))\n",
    "# print(res.shape)\n",
    "# # print(acts.unsqueeze(-1)[inds == 6].shape)\n",
    "# src = torch.tensor(range(10), dtype=acts.dtype, device=acts.device).repeat((6))\n",
    "# print(src.shape)\n",
    "# acts[inds == 6] = src\n",
    "# print(acts.shape)\n",
    "# acts\n",
    "# vals.squeeze().shape\n",
    "# print(f\"{acts.shape}, {inds.shape}, {vals.shape}\")\n",
    "\n",
    "# a = torch.reshape(torch.tensor(range(40)), (4, 10))\n",
    "# inds = torch.tensor([[2, 3], [6, 7], [0, 1], [4, 5]])\n",
    "# inds == 2\n",
    "# b = torch.gather(a, 1, inds)\n",
    "# b[inds == 2]\n",
    "\n",
    "# inds = inds.repeat((1, output.shape[-1]))\n",
    "# output = torch.reshape(output[inds == 19], (-1, output.shape[-1]))\n",
    "# avgs = torch.mean(output, 0, keepdim=True)\n",
    "# avgs.shape\n",
    "# output.shape\n",
    "\n",
    "# meta_net._activations['fc'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78bc5cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_net.templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6682daf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 10\n",
    "# tik = time.time()\n",
    "# launch(num_epochs=num_epochs, num_proc=2)\n",
    "# tok = time.time()\n",
    "# tok - tik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38780db1",
   "metadata": {},
   "outputs": [],
   "source": [
    " # vit_net = SimpleViT(\n",
    " #     image_size = 32,\n",
    " #     patch_size = 8,\n",
    " #     num_classes = len(classes),\n",
    " #     dim = 1024,\n",
    " #     depth = 6,\n",
    " #     heads = 16,\n",
    " #     mlp_dim = 2048\n",
    " # )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
