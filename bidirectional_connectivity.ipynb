{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d943e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb53540",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "D = 6\n",
    "K = 2\n",
    "X = np.zeros((N, D))\n",
    "X[:N//2, :D//2] = 1\n",
    "X[N//2:, D//2:] = 1\n",
    "Y = np.zeros((N, K))\n",
    "Y[:N//2, :K//2] = 1\n",
    "Y[N//2:, K//2:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 2\n",
    "W1_i = np.random.normal(size=(D, H))\n",
    "W1_i = W1_i / np.linalg.norm(W1_i)\n",
    "W1_o = np.random.normal(size=(H, D))\n",
    "W1_o = W1_o / np.linalg.norm(W1_o)\n",
    "pprint(W1_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0272c34c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# W1 = W1_i@W1_o\n",
    "# W1 = W1 / np.linalg.norm(W1)\n",
    "# res = X\n",
    "# prev_res = 5*X\n",
    "# iters = 0\n",
    "# diff = np.linalg.norm(res - prev_res)\n",
    "# while diff > 1e-4:\n",
    "#     prev_res = res\n",
    "#     res = res@W1\n",
    "#     res = np.maximum(res, 0)\n",
    "#     res = res / np.linalg.norm(res, axis=1)[:, np.newaxis]\n",
    "#     diff = np.linalg.norm(res - prev_res)\n",
    "#     iters += 1\n",
    "# res = res@W1_i\n",
    "# pprint(iters)\n",
    "# pprint(diff)\n",
    "# pprint(prev_res)\n",
    "# pprint(res)\n",
    "# pprint(X@W1_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdda78d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1_stacked = np.vstack([W1_i, W1_o.T])\n",
    "W1_joint = W1_stacked @ W1_stacked.T\n",
    "Y_priors = np.zeros((N, K))\n",
    "X_hats = Y_priors @ W1_o\n",
    "X_aug = np.hstack([X, X_hats])\n",
    "rep = X_aug\n",
    "prev_rep = 5*rep # arbitrary to make the diff large\n",
    "iters = 0\n",
    "diff = np.linalg.norm(rep - prev_rep)\n",
    "while diff > 1e-4:\n",
    "    prev_rep = rep\n",
    "    rep = X_aug @ W1_joint\n",
    "    rep = np.maximum(rep, 0)\n",
    "    rep = rep / np.linalg.norm(rep, axis=1)[:, np.newaxis]\n",
    "    diff = np.linalg.norm(rep - prev_rep)\n",
    "    iters +=1\n",
    "pprint(iters)\n",
    "pprint(diff)\n",
    "pprint(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ebe3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec47c030",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetFC(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(NetFC, self).__init__()\n",
    "        self.W1_i = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.W1_i(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724b9b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiNetFC(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(BiNetFC, self).__init__()\n",
    "        self.W1_i = nn.Linear(input_dim, output_dim)\n",
    "        self.W1_o = nn.Linear(output_dim, input_dim)\n",
    "        self.thresh = 1e-4\n",
    "\n",
    "    def forward(self, X):\n",
    "        print(X)\n",
    "        rep = X / torch.linalg.norm(X, dim=1, keepdim=True)\n",
    "        prev_rep = 5*rep\n",
    "        diff = torch.linalg.norm(rep - prev_rep)\n",
    "        iters = 0\n",
    "        print(rep)\n",
    "        while diff > self.thresh:\n",
    "#             print(iters)\n",
    "#             print(diff)\n",
    "            prev_rep = rep\n",
    "            rep = self.W1_i(rep)\n",
    "            rep = F.relu(rep)\n",
    "            rep = self.W1_o(rep)\n",
    "            rep = F.relu(rep)\n",
    "            rep = rep / torch.linalg.norm(rep, dim=1, keepdim=True)\n",
    "            diff = torch.linalg.norm(rep - prev_rep) \n",
    "            iters += 1\n",
    "            print(rep)\n",
    "        rep = self.W1_i(rep)\n",
    "        print(iters)\n",
    "        output = F.log_softmax(rep, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b70fae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 10\n",
    "D = 6\n",
    "K = 2\n",
    "X = np.zeros((N, D))\n",
    "X[:N//2, :D//2] = 1\n",
    "X[N//2:, D//2:] = 1\n",
    "Y = np.zeros((N, K))\n",
    "Y[:N//2, :K//2] = 1\n",
    "Y[N//2:, K//2:] = 1\n",
    "\n",
    "X = torch.from_numpy(X).to(torch.float32)\n",
    "Y = torch.from_numpy(Y).to(torch.float32)\n",
    "net = BiNetFC(D, K)\n",
    "for _ in range(0):\n",
    "    Y_hat = net(X)\n",
    "    loss = ((torch.exp(Y_hat) - Y)**2).sum()\n",
    "    loss.backward()\n",
    "    for p in net.parameters():\n",
    "#         print(p.grad)\n",
    "        p.data.add_(- 0.001 * p.grad)\n",
    "        p.grad.data.zero_()\n",
    "Y_hat = net(X)\n",
    "print(Y_hat)\n",
    "torch.exp(Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5de223",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, p in net.named_parameters():\n",
    "    print(name, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f146fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiNet2FC(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, convergence_thresh):\n",
    "        super(BiNet2FC, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        in_dim = input_dim\n",
    "        aux_dim = hidden_dim\n",
    "        self.layers = []\n",
    "        for layer_idx in range(num_layers):\n",
    "            if layer_idx > 0:\n",
    "                in_dim = hidden_dim\n",
    "            if layer_idx == num_layers - 1:\n",
    "                aux_dim = output_dim\n",
    "            fc_layer = nn.Linear(in_dim + aux_dim, hidden_dim)\n",
    "            self.layers.append(fc_layer)\n",
    "        self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        self.thresh = convergence_thresh\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size = X.shape[0]\n",
    "        prev_reps = []\n",
    "        for layer_idx in range(len(self.layers)):\n",
    "            layer = self.layers[layer_idx]\n",
    "            placeholder_rep = torch.zeros(batch_size, layer.out_features)\n",
    "            prev_reps.append(placeholder_rep)\n",
    "        reps = []\n",
    "        diff = float('inf')\n",
    "        iters = 0\n",
    "        while diff > self.thresh:\n",
    "            diff = 0.\n",
    "            reps = []\n",
    "            input_lower = X\n",
    "            for layer_idx in range(len(self.layers)):\n",
    "                if layer_idx > 0:\n",
    "                    input_lower = reps[layer_idx - 1]\n",
    "                input_aug = input_lower\n",
    "                if layer_idx < len(self.layers) - 1:\n",
    "                    input_upper = prev_reps[layer_idx + 1]\n",
    "                    input_aug = torch.hstack([input_lower, input_upper])\n",
    "                rep = F.relu(self.layers[layer_idx](input_aug))\n",
    "                reps.append(rep)\n",
    "                layer_diff = torch.linalg.norm(rep - prev_reps[layer_idx])\n",
    "                diff += layer_diff\n",
    "            prev_reps = reps\n",
    "            diff /= len(self.layers)\n",
    "            iters += 1\n",
    "#         print(f\"forward iterations: {iters}\")\n",
    "        output = F.log_softmax(reps[-1], dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ae80b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 10\n",
    "D = 6\n",
    "H = 3\n",
    "K = 2\n",
    "X = np.zeros((N, D))\n",
    "X[:N//2, :D//2] = 1\n",
    "X[N//2:, D//2:] = 1\n",
    "Y = np.zeros((N, K))\n",
    "Y[:N//2, :K//2] = 1\n",
    "Y[N//2:, K//2:] = 1\n",
    "\n",
    "X = torch.from_numpy(X).to(torch.float32)\n",
    "Y = torch.from_numpy(Y).to(torch.float32)\n",
    "net = BiNet2FC(D, H, K, 3, 1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "loss = torch.zeros(1)\n",
    "prev_loss = torch.tensor(float('inf'))\n",
    "\n",
    "iters = 0\n",
    "while torch.linalg.norm(loss - prev_loss) > 1e-4:\n",
    "    Y_hat = net(X)\n",
    "    prev_loss = loss\n",
    "    loss = loss_fn(Y_hat, Y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    iters += 1\n",
    "    if iters % 20 == 0:\n",
    "        print(loss)\n",
    "print(iters)\n",
    "Y_hat = net(X)\n",
    "print(net)\n",
    "preds = torch.exp(Y_hat)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22a69eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a986ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a48463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiNetConv(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, convergence_thresh=1e-3, iter_limit=100, device=torch.device(\"cpu\")):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        in_dim = input_dim\n",
    "        aux_dim = hidden_dim\n",
    "        self.conv_layers = []\n",
    "        self.bn_layers = []\n",
    "        for layer_idx in range(num_layers):\n",
    "            if layer_idx > 0:\n",
    "                in_dim = hidden_dim\n",
    "            if layer_idx == num_layers - 1:\n",
    "                aux_dim = 0\n",
    "            conv_layer = nn.Conv2d(in_dim + aux_dim, hidden_dim, 5, padding='same')\n",
    "            self.conv_layers.append(conv_layer)\n",
    "            self.bn_layers.append(nn.BatchNorm2d(self.hidden_dim))\n",
    "        self.conv_layers = nn.ModuleList(self.conv_layers)\n",
    "        self.bn_layers = nn.ModuleList(self.bn_layers)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 16 * 16, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, output_dim)\n",
    "        self.thresh = convergence_thresh\n",
    "        self.iter_limit = iter_limit\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, X):\n",
    "        batch_size = X.shape[0]\n",
    "        prev_reps = []\n",
    "        for layer_idx in range(len(self.conv_layers)):\n",
    "            conv_layer = self.conv_layers[layer_idx]\n",
    "            placeholder_rep = torch.zeros(batch_size, self.hidden_dim, 32, 32, device=self.device)\n",
    "            prev_reps.append(placeholder_rep)\n",
    "        reps = []\n",
    "        diff = float('inf')\n",
    "        iters = 0\n",
    "        while diff > self.thresh and iters < self.iter_limit:\n",
    "#             print(f\"number of iterations: {iters}\")\n",
    "#             print(f\"diff: {diff}\")\n",
    "            diff = 0.\n",
    "            reps = []\n",
    "            input_lower = X\n",
    "            for layer_idx in range(len(self.conv_layers)):\n",
    "                if layer_idx > 0:\n",
    "                    input_lower = reps[layer_idx - 1]\n",
    "                input_aug = input_lower\n",
    "                if layer_idx < len(self.conv_layers) - 1:\n",
    "                    input_upper = prev_reps[layer_idx + 1]\n",
    "                    input_aug = torch.hstack([input_lower, input_upper])\n",
    "                rep = F.relu(self.conv_layers[layer_idx](input_aug))\n",
    "                rep = self.bn_layers[layer_idx](rep)\n",
    "                reps.append(rep)\n",
    "                layer_diff = torch.linalg.norm(rep - prev_reps[layer_idx])\n",
    "                diff += layer_diff\n",
    "            prev_reps = reps\n",
    "            diff /= len(self.conv_layers)\n",
    "            iters += 1\n",
    "#         print(f\"number of iterations: {iters}\")\n",
    "#         print(f\"diff: {diff}\")\n",
    "        rep = self.pool(rep)\n",
    "        x = torch.flatten(rep, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "#         batch_size = X.shape[0]\n",
    "#         prev_reps = []\n",
    "#         for layer_idx in range(len(self.layers)):\n",
    "#             layer = self.layers[layer_idx]\n",
    "#             placeholder_rep = torch.zeros(batch_size, layer.out_features)\n",
    "#             prev_reps.append(placeholder_rep)\n",
    "#         reps = []\n",
    "#         diff = float('inf')\n",
    "#         iters = 0\n",
    "#         while diff > self.thresh:\n",
    "#             diff = 0.\n",
    "#             reps = []\n",
    "#             input_lower = X\n",
    "#             for layer_idx in range(len(self.layers)):\n",
    "#                 if layer_idx > 0:\n",
    "#                     input_lower = reps[layer_idx - 1]\n",
    "#                 input_aug = input_lower\n",
    "#                 if layer_idx < len(self.layers) - 1:\n",
    "#                     input_upper = prev_reps[layer_idx + 1]\n",
    "#                     input_aug = torch.hstack([input_lower, input_upper])\n",
    "#                 rep = F.relu(self.layers[layer_idx](input_aug))\n",
    "#                 reps.append(rep)\n",
    "#                 layer_diff = torch.linalg.norm(rep - prev_reps[layer_idx])\n",
    "#                 diff += layer_diff\n",
    "#             prev_reps = reps\n",
    "#             diff /= len(self.layers)\n",
    "#             iters += 1\n",
    "# #         print(f\"forward iterations: {iters}\")\n",
    "#         output = F.log_softmax(reps[-1], dim=1)\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acecaa0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 400\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "net = BiNetConv(3, 16, 10, 4, convergence_thresh=1e-4, iter_limit=10, device=device)\n",
    "print(net)\n",
    "net.to(device)\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = dataiter.next()\n",
    "# images, labels = images.to(device), labels.to(device)\n",
    "# output = net(images)\n",
    "# print(output.shape)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "epochs = 10\n",
    "net.train()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        num_mb = 25\n",
    "        if i % num_mb == num_mb-1:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / num_mb:.3f}')\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa6528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    iters = 0\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = net(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        iters += 1\n",
    "        \n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d56bae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
